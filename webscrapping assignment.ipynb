{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526be245",
   "metadata": {},
   "source": [
    "# webscrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8dae27",
   "metadata": {},
   "source": [
    "1) Write a python program to display all the header tags from wikipedia.org and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec7833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Header\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_wikipedia_headers(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "        \n",
    "        # Extract the text from each header tag and store it in a list\n",
    "        header_texts = [header.get_text() for header in headers]\n",
    "        \n",
    "        # Create a DataFrame from the list of header texts\n",
    "        df = pd.DataFrame({'Header': header_texts})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wikipedia_url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "    header_df = get_wikipedia_headers(wikipedia_url)\n",
    "    if header_df is not None:\n",
    "        print(header_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420df503",
   "metadata": {},
   "source": [
    "2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fa539b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the web page. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_former_presidents_data(url):\n",
    "    # Send a GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the table containing the information about former presidents\n",
    "        table = soup.find('table', {'class': 'table table-bordered table-striped'})\n",
    "        \n",
    "        # Initialize empty lists to store the data\n",
    "        names = []\n",
    "        terms_of_office = []\n",
    "        \n",
    "        # Extract data from the table\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows[1:]:  # Skip the header row\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) == 2:\n",
    "                name = columns[0].get_text(strip=True)\n",
    "                term_of_office = columns[1].get_text(strip=True)\n",
    "                names.append(name)\n",
    "                terms_of_office.append(term_of_office)\n",
    "        \n",
    "        df = pd.DataFrame({'Name': names, 'Term of Office': terms_of_office})\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the web page. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    presidents_url = \"https://presidentofindia.nic.in/former-presidents.html\"\n",
    "    presidents_df = get_former_presidents_data(presidents_url)\n",
    "    if presidents_df is not None:\n",
    "        print(presidents_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7112e04",
   "metadata": {},
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "c) Top 10 ODI bowlers along with the records of their team andrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0146b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7428\\2217265066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mtop_10_teams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_10_batsmen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_10_bowlers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_icc_cricket_rankings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtop_10_teams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7428\\2217265066.py\u001b[0m in \u001b[0;36mscrape_icc_cricket_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mteams_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mteams_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'table rankings-table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mteams_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tbody'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mteam_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data and create a DataFrame\n",
    "def scrape_icc_cricket_rankings(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch data from the ICC website.\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract data for the top 10 ODI teams\n",
    "    teams_data = []\n",
    "    teams_table = soup.find('table', class_='table rankings-table')\n",
    "    for row in teams_table.find('tbody').find_all('tr')[:10]:\n",
    "        cols = row.find_all('td')\n",
    "        team_name = cols[1].text.strip()\n",
    "        matches = int(cols[2].text.strip())\n",
    "        points = int(cols[3].text.strip())\n",
    "        rating = int(cols[4].text.strip())\n",
    "        teams_data.append([team_name, matches, points, rating])\n",
    "\n",
    "    # Extract data for the top 10 ODI batsmen\n",
    "    batsmen_data = []\n",
    "    batsmen_table = soup.find_all('table', class_='table rankings-table')[1]\n",
    "    for row in batsmen_table.find('tbody').find_all('tr')[:10]:\n",
    "        cols = row.find_all('td')\n",
    "        player_name = cols[1].text.strip()\n",
    "        team_name = cols[2].text.strip()\n",
    "        rating = int(cols[3].text.strip())\n",
    "        batsmen_data.append([player_name, team_name, rating])\n",
    "\n",
    "    # Extract data for the top 10 ODI bowlers\n",
    "    bowlers_data = []\n",
    "    bowlers_table = soup.find_all('table', class_='table rankings-table')[2]\n",
    "    for row in bowlers_table.find('tbody').find_all('tr')[:10]:\n",
    "        cols = row.find_all('td')\n",
    "        player_name = cols[1].text.strip()\n",
    "        team_name = cols[2].text.strip()\n",
    "        rating = int(cols[3].text.strip())\n",
    "        bowlers_data.append([player_name, team_name, rating])\n",
    "\n",
    "    # Create DataFrames\n",
    "    teams_df = pd.DataFrame(teams_data, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
    "    batsmen_df = pd.DataFrame(batsmen_data, columns=['Player', 'Team', 'Rating'])\n",
    "    bowlers_df = pd.DataFrame(bowlers_data, columns=['Player', 'Team', 'Rating'])\n",
    "\n",
    "    return teams_df, batsmen_df, bowlers_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "    top_10_teams, top_10_batsmen, top_10_bowlers = scrape_icc_cricket_rankings(url)\n",
    "\n",
    "    if top_10_teams is not None:\n",
    "        print(\"Top 10 ODI Teams:\")\n",
    "        print(top_10_teams)\n",
    "    \n",
    "    if top_10_batsmen is not None:\n",
    "        print(\"\\nTop 10 ODI Batsmen:\")\n",
    "        print(top_10_batsmen)\n",
    "\n",
    "    if top_10_bowlers is not None:\n",
    "        print(\"\\nTop 10 ODI Bowlers:\")\n",
    "        print(top_10_bowlers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7894db",
   "metadata": {},
   "source": [
    "Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5cab12b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7428\\4239852256.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mtop_10_teams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_10_batting_players\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_10_all_rounders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_icc_womens_cricket_rankings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtop_10_teams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7428\\4239852256.py\u001b[0m in \u001b[0;36mscrape_icc_womens_cricket_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mteams_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mteams_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'table'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'table rankings-table'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mteams_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tbody'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mteam_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data and create a DataFrame\n",
    "def scrape_icc_womens_cricket_rankings(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch data from the ICC website.\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract data for the top 10 women's ODI teams\n",
    "    teams_data = []\n",
    "    teams_table = soup.find('table', class_='table rankings-table')\n",
    "    for row in teams_table.find('tbody').find_all('tr')[:10]:\n",
    "        cols = row.find_all('td')\n",
    "        team_name = cols[1].text.strip()\n",
    "        matches = int(cols[2].text.strip())\n",
    "        points = int(cols[3].text.strip())\n",
    "        rating = int(cols[4].text.strip())\n",
    "        teams_data.append([team_name, matches, points, rating])\n",
    "\n",
    "    # Extract data for the top 10 women's ODI batting players\n",
    "    batting_players_data = []\n",
    "    batting_players_table = soup.find_all('table', class_='table rankings-table')[1]\n",
    "    for row in batting_players_table.find('tbody').find_all('tr')[:10]:\n",
    "        cols = row.find_all('td')\n",
    "        player_name = cols[1].text.strip()\n",
    "        team_name = cols[2].text.strip()\n",
    "        rating = int(cols[3].text.strip())\n",
    "        batting_players_data.append([player_name, team_name, rating])\n",
    "\n",
    "    # Extract data for the top 10 women's ODI all-rounders\n",
    "    all_rounders_data = []\n",
    "    all_rounders_table = soup.find_all('table', class_='table rankings-table')[3]\n",
    "    for row in all_rounders_table.find('tbody').find_all('tr')[:10]:\n",
    "        cols = row.find_all('td')\n",
    "        player_name = cols[1].text.strip()\n",
    "        team_name = cols[2].text.strip()\n",
    "        rating = int(cols[3].text.strip())\n",
    "        all_rounders_data.append([player_name, team_name, rating])\n",
    "\n",
    "    # Create DataFrames\n",
    "    teams_df = pd.DataFrame(teams_data, columns=['Team', 'Matches', 'Points', 'Rating'])\n",
    "    batting_players_df = pd.DataFrame(batting_players_data, columns=['Player', 'Team', 'Rating'])\n",
    "    all_rounders_df = pd.DataFrame(all_rounders_data, columns=['Player', 'Team', 'Rating'])\n",
    "\n",
    "    return teams_df, batting_players_df, all_rounders_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "    top_10_teams, top_10_batting_players, top_10_all_rounders = scrape_icc_womens_cricket_rankings(url)\n",
    "\n",
    "    if top_10_teams is not None:\n",
    "        print(\"Top 10 Women's ODI Teams:\")\n",
    "        print(top_10_teams)\n",
    "    \n",
    "    if top_10_batting_players is not None:\n",
    "        print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "        print(top_10_batting_players)\n",
    "\n",
    "    if top_10_all_rounders is not None:\n",
    "        print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "        print(top_10_all_rounders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c77f2c",
   "metadata": {},
   "source": [
    "5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame\u0002i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f87a71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape news details and create a DataFrame\n",
    "def scrape_cnbc_news(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch data from CNBC website.\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract news details\n",
    "    headlines = []\n",
    "    times = []\n",
    "    news_links = []\n",
    "\n",
    "    # Find the elements containing news details\n",
    "    news_items = soup.find_all('div', class_='Card-text___1B9AM')\n",
    "\n",
    "    for item in news_items:\n",
    "        headline = item.find('h3', class_='Card-title___3jRQm').text.strip()\n",
    "        time = item.find('time').text.strip()\n",
    "        news_link = item.find('a', class_='Card-titleLink___LJf5H')['href']\n",
    "        \n",
    "        headlines.append(headline)\n",
    "        times.append(time)\n",
    "        news_links.append(news_link)\n",
    "\n",
    "    # Create a DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda58a01",
   "metadata": {},
   "source": [
    "6) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\u0002i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6384583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape article details and create a DataFrame\n",
    "def scrape_ai_journal_articles(url):\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch data from Elsevier AI journal.\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract article details\n",
    "    paper_titles = []\n",
    "    authors_list = []\n",
    "    published_dates = []\n",
    "    paper_urls = []\n",
    "\n",
    "    # Find the elements containing article details\n",
    "    articles = soup.find_all('div', class_='pod-listing')\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.find('a', class_='pod-listing-title')\n",
    "        paper_title = title.text.strip()\n",
    "\n",
    "        authors = article.find('div', class_='text-xs')\n",
    "        authors_text = authors.text.strip()\n",
    "\n",
    "        date = article.find('div', class_='pod-listing-meta')\n",
    "        published_date = date.text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e43421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
